__Nuclear Submarine Test__

_Focus on evaluating the process not the product (reproducible is about whole workflow not just final output)_

_Inspirational - place to start discussion not to get a score - can expand out to more detailed test like UW score card_

Big Goal: Can an external reviewer, who you have never met, recreate your outputs from the materials that you have provided?

Questions for Reflection:

1.  Are all of the steps in the pipeline that translates data to output (figures/tables) documented?  [no holes/leaks in pipeline]
2.  How much of the pipeline that translates data to output (figures/tables) is automated? [Make/“run_everything.py”/”make_figures.py”]
3.  Is the data [publicly] accessible and documented?
4.  Is the code [publicly] accessible and documented?
5.  Are the data and code tested and tests documented (how)? [not just unit test, could be manual]
6.  Have you tracked the history of all your data/steps/revisions? [provenance]
7.  Is your raw and final output (publication) available and open access?
8.  Can you verify the reliability of the tools that you’ve used? [open source?]
9.  Have others reviewed your process or code? [peer reviewed]
10. If someone found an error in your work, could you easily identify the cause and fix it?
