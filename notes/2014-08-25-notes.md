
# Notes from first reproducibility wg meeting
(Philip, Karthik, and Fatma)
_August 25th, 2014_

__Standardize vocabulary: What is reproducibility?__
Need a standard lexicon.

##Why do we care about reproducibility?
1. Evidence of correctness
2. Re-use
3. Vary inputs/methods to determine robustness of results
4. Methods/techniques as objects of research/study and sources of ideas

- Clarify ambuiguity in overloaded terminology (reproducibility, repeatability, replicability, verifiability, auditability, ... mean different things in different domains)
- Open science, open data, open-source code, and reproducible research are often lumped together.
  While they share many features, they are separate.


__What makes a research study reproducible?__

- Establish domain-specific criteria to classify where a research study falls on the
  reproducibility spectrum (impossible to reproduce â†’  fully reproducible complete with full computing environment).
  >>_Note: this language is biased towards computational reproducibility, not other flavors of reproducibility_
- Can we establish a checklist or a "decision tree" that would place a study somewhere on this spectrum? E.g., if you
are analyzing your data with a point-and-click interface (such as a spreadsheet), do not pass "go."
- Can can we adaptively suggest remedies for the biggest gaps?
- Can we establish a set of recommendations for each level (e.g., incorporating version control and two other simple (kidding) steps will make your study more reproducible).

__Reproducibility is a practice, not a particular set of tools__

- Of course, many tools make it easier to work reproducibly.

__What are the incentives reproducibility?__

Research papers are merely advertisements for the real scholarship, which is often never shared (paraphrasing David Donoho). How do we incentivize good scientific practice over the current norm?

- Why would researchers take the time and effort to make their work reproducible when those activities are not recognized in academia? (ties into the careers and incentives wg).
- Can we identify any progress on this front that might benefit researchers?
    + [Code as a research object](http://mozillascience.org/code-as-a-research-object-a-new-project/) (GitHub + Mozilla Science Lab)
    + [Altmetrics](http://altmetrics.org/manifesto/) (see ALM, and services like [ImpactStory](https://impactstory.org/))
    + Partnering with journals and badging papers (as evaluated by reviewers) that are minimally reproducible (something we could do).

__Identify training gaps__

- What skills do researchers need to make their existing work more reproducible?
    + What skills do we teach early career researchers (who still have time to train) versus established researchers? Semester long courses versus workshops?
    + Can BIDS play a role in teaching these as a series of small workshops (e.g. version control as a separate workshop) (ties into training and curriculum wg)

__What are the software components in various scientific environments?__

- Emphasize need for revision control
- Emphasize inappropriateness of some tools, such as spreadsheets
- At least with the two major scientific programming environments (R and Python), can we identify tools and workflows that researchers can incorporate to make their work more reproducible (ties into the Software WG)
